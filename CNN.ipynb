{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7485ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D, Conv3D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# Our dictionary will contain only of the top 7000 words appearing most frequently\n",
    "\n",
    "# Now we split our data-set into training and test data\n",
    "\n",
    "# Looking at the nature of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b183d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imports as ii\n",
    "import functions as f\n",
    "import preprocessing as pp\n",
    "import neuralnetworks as nn\n",
    "import trainRNN as trainRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67a333ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n",
      "Number of data:  10000\n"
     ]
    }
   ],
   "source": [
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test.json.gz\"\n",
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "test = f.readJson(PATH[\"music_reviews_dev\"])\n",
    "test_true = f.readJson(PATH[\"music_reviews_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "051e2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, train_idx, train_missing_idx = f.json_divide(train)\n",
    "X_dev, y_dev, test_idx, test_missing_idx = f.json_divide(test)\n",
    "X_test, y_test, test_idx, test_missing_idx = f.json_divide(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c58aba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = [1, 1, 1, 1, 1, 1]\n",
    "combination = [1, 1, 0, 1, 1, 0]\n",
    "combination = [0, 1, 0, 0, 0, 0]\n",
    "data_sets, y_train, y_test = f.grid_search_retrain(X_train, X_test, y_train, y_test, combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2cd9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_sets[0][1]\n",
    "X_test = data_sets[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "80486dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pp.tokenizer_init(X_train, X_test, X_test)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a685e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c7bc78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the data samples to a maximum review length in words\n",
    "max_words = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# Building the CNN Model\n",
    "model = Sequential()      # initilaizing the Sequential nature for CNN model\n",
    "# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output of those words which belong in the top_words dictionary\n",
    "model.add(Embedding(vocabulary_size+1, 32, input_length=max_words))\n",
    "model.add(Conv1D(32, 1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1564c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 100, 32)           2336448   \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 100, 32)           1056      \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 50, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 250)               400250    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,738,005\n",
      "Trainable params: 2,738,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "811dac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = {\"positive\": 1, \"negative\": 0}\n",
    "y_train = pp.sentiment_converter(y_train, sent_dict)\n",
    "y_dev = pp.sentiment_converter(y_dev, sent_dict)\n",
    "y_test = pp.sentiment_converter(y_test, sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eff343b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_test = np.array(X_test)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83db9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.2846 - accuracy: 0.8815 - val_loss: 0.2363 - val_accuracy: 0.9103\n",
      "Epoch 2/4\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.1796 - accuracy: 0.9340 - val_loss: 0.2529 - val_accuracy: 0.9025\n",
      "Epoch 3/4\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.1284 - accuracy: 0.9552 - val_loss: 0.2895 - val_accuracy: 0.8949\n",
      "Epoch 4/4\n",
      "1000/1000 [==============================] - 48s 48ms/step - loss: 0.0892 - accuracy: 0.9705 - val_loss: 0.3720 - val_accuracy: 0.8895\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3720 - accuracy: 0.8895\n",
      "Accuracy: 88.95%\n"
     ]
    }
   ],
   "source": [
    "# Fitting the data onto model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=100, verbose=1)\n",
    "# Getting score metrics from our model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# Displays the accuracy of correct sentiment prediction over test data\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330e9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
