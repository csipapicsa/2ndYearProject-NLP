{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3874c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jasro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jasro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jasro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jasro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  100000\n",
      "Number of data:  10000\n",
      "Number of data:  10000\n",
      "[4603, 4838, 16019, 18229, 19785, 23315, 28397, 28834, 33359, 43138, 43539, 43690, 44008, 44477, 44972, 48811, 49317, 50106, 51717, 52286, 55555, 56171, 57223, 58328, 58799, 58866, 59525, 59739, 61046, 61914, 61916, 62831, 63208, 72268, 78944, 79067, 80093, 80637, 80658, 81640, 81900, 82510, 83138, 83145, 83615, 84761, 87112, 88895, 88984, 89132, 91949, 94301, 94727, 99641]\n",
      "[2900, 4294, 5135, 8540]\n",
      "train_sent length 99946\n",
      "dev_sent length 9996\n",
      "every string is 40 bytes on it's own, and then 1 byte pr character\n",
      "no. of characters in dev_sent: 2427461\n",
      "40*9996 + 2427461 = 2827301 which is 2.8 megabytes\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install symspellpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import urllib.request\n",
    "import csv\n",
    "import importlib \n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "# !pip install contractions\n",
    "# !pip install tqdm\n",
    "f = importlib.import_module('functions')\n",
    "\n",
    "PATH = {}\n",
    "PATH[\"dataset_classification\"] = \"dataset/classification/\"\n",
    "PATH[\"dataset_labeling\"] = \"dataset/seq_labeling/\"\n",
    "PATH[\"music_reviews_train\"] = PATH[\"dataset_classification\"] + \"music_reviews_train.json.gz\"\n",
    "PATH[\"music_reviews_dev\"] = PATH[\"dataset_classification\"] + \"music_reviews_dev.json.gz\"\n",
    "PATH[\"music_reviews_test\"] = PATH[\"dataset_classification\"] + \"music_reviews_test_masked.json.gz\"\n",
    "PATH[\"hard_sentences\"] = PATH[\"dataset_classification\"] + \"hard_sentences.json.gz\"\n",
    "\n",
    "train = f.readJson(PATH[\"music_reviews_train\"])\n",
    "dev = f.readJson(PATH[\"music_reviews_dev\"])\n",
    "test = f.readJson(PATH[\"music_reviews_test\"])\n",
    "\n",
    "sent_dict = {\"positive\": 1, \"negative\": 0, \"POSITIVE\": 1, \"NEGATIVE\":0}\n",
    "\n",
    "# read the train data\n",
    "data = train\n",
    "train_sent = []\n",
    "train_sentiment = []\n",
    "train_idx = []\n",
    "train_missing_indexies = []\n",
    "y_train = []\n",
    "length_of_sentencies_counter = []\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        train_sent.append(data[i][\"reviewText\"])\n",
    "        train_sentiment.append(data[i][\"sentiment\"])\n",
    "        train_idx.append(i)\n",
    "        y_train.append(sent_dict[data[i][\"sentiment\"]])\n",
    "        length_of_sentencies_counter.append(len(data[i][\"reviewText\"].split()))\n",
    "    except KeyError:\n",
    "        train_missing_indexies.append(i)\n",
    "        continue\n",
    "print(train_missing_indexies)\n",
    "\n",
    "# read the dev data \n",
    "data = dev\n",
    "dev_sent = []\n",
    "dev_sentiment = []\n",
    "dev_idx = []\n",
    "dev_missing_indexies = []\n",
    "dev_y_train = []\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        dev_sent.append(data[i][\"reviewText\"])\n",
    "        dev_sentiment.append(data[i][\"sentiment\"])\n",
    "        dev_idx.append(i)\n",
    "        dev_y_train.append(sent_dict[data[i][\"sentiment\"]]) \n",
    "    except KeyError:\n",
    "        dev_missing_indexies.append(i)\n",
    "        continue\n",
    "print(dev_missing_indexies)\n",
    "\n",
    "\n",
    "\n",
    "# make dev_sentiment into vector for checking accuracy laters... \n",
    "dev_classvec = np.array([sent_dict[s] for s in dev_sentiment])\n",
    "\n",
    "def printlen(a, a_name): print(f'{a_name} length {len(a)}')\n",
    "printlen(train_sent, \"train_sent\")\n",
    "printlen(dev_sent, \"dev_sent\")\n",
    "\n",
    "charactercount = 0\n",
    "for sentence in dev_sent:\n",
    "    charactercount += len(sentence)\n",
    "print(\"every string is 40 bytes on it's own, and then 1 byte pr character\")\n",
    "print(f'no. of characters in dev_sent: {charactercount}')\n",
    "print(\"40*9996 + 2427461 = 2827301 which is 2.8 megabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b57322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794d439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8497f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from \"positive\" to 1, \"negative\" to 0\n",
    "dev_sentiment = [sent_dict[ds] for ds in dev_sentiment] \n",
    "train_sentiment = [sent_dict[ts] for ts in train_sentiment]\n",
    "\n",
    "dev = [dev_sent, dev_sentiment]\n",
    "train = [train_sent, train_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f1d47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputExample(guid=None, text_a='My dentist recommended this as a relaxation technique for dental visits. They give me an ipod with headphones, play this on it and it relieves some of the stress of dental treatment, which I dislike intensely.\\nIt worked so well that I bought my own copy to try at home. I fall asleep after a couple of minutes and stay asleep. Instead of tossing and turning, I hardly move at all. Highly recommend.', text_b=None, label=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples = [InputExample(guid=None, text_a=r, label=s) for r,s in zip(train_sent, train_sentiment)]\n",
    "validation_InputExamples = [InputExample(guid=None, text_a=r, label=s) for r,s in zip(dev_sent, dev_sentiment)]\n",
    "validation_InputExamples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ceb65",
   "metadata": {},
   "source": [
    "Notes: \n",
    "BatchEncoding holds the output of the tokenizer’s encoding methods (__call__, encode_plus and batch_encode_plus) and is derived from a Python dictionary. When the tokenizer is a pure python tokenizer, this class behave just like a standard python dictionary and hold the various model inputs computed by these methodes (input_ids, attention_mask…). When the tokenizer is a “Fast” tokenizer (i.e. backed by HuggingFace tokenizers library), this class provides in addition several advanced alignement methods which can be used to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4ba0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## \n",
    "##############\n",
    "############## inspiration from https://www.kaggle.com/code/satyampd/imdb-sentiment-analysis-using-bert-w-huggingface/notebook\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=40):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in tqdm(examples): # progress bar\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n",
    "            max_length=max_length,    # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'review' # TODO delete? \n",
    "LABEL_COLUMN = 'sentiment' # TODO delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a7505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 6666\n",
    "no = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa725907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/5000 [00:00<?, ?it/s]C:\\Users\\jasro\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 754.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_data = convert_examples_to_tf_dataset(train_InputExamples[:5000], tokenizer)\n",
    "train_data = convert_examples_to_tf_dataset(train_InputExamples[start:(start+no)], tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2) # tf stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7865f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9996/9996 [00:11<00:00, 846.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# validation_data = convert_examples_to_tf_dataset(validation_InputExamples[:1000], tokenizer)\n",
    "validation_data = convert_examples_to_tf_dataset(validation_InputExamples, tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cec2c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf026d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the datasets, but outcommented cuz it's already done \n",
    "\n",
    "# tf.data.experimental.save(\n",
    "#     train_data, \"dataset/bert_tokenized/train_data_6666_5k\", compression=None, shard_func=None, checkpoint_args=None\n",
    "# )\n",
    "# tf.data.experimental.save(\n",
    "#     validation_data, \"dataset/bert_tokenized/validation_data_6666_5k\", compression=None, shard_func=None, checkpoint_args=None\n",
    "# )\n",
    "\n",
    "# TO RETRIEVE\n",
    "train_data = tf.data.experimental.load(\"dataset/bert_tokenized/train_data_6666_5k\", element_spec=None, compression=None, reader_func=None)\n",
    "validation_data = tf.data.experimental.load(\"dataset/bert_tokenized/validation_data_6666_5k\", element_spec=None, compression=None, reader_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69cb8d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "314/314 [==============================] - 2717s 9s/step - loss: 0.2531 - accuracy: 0.8959 - val_loss: 0.3142 - val_accuracy: 0.8932\n",
      "Epoch 2/2\n",
      "314/314 [==============================] - 2870s 9s/step - loss: 0.0842 - accuracy: 0.9733 - val_loss: 0.3940 - val_accuracy: 0.8975\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "history = model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c10097d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 16:12:34,770 WARNING:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1050). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/bert_uncased_trained5k_maxlen40_final\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 16:12:41,746 INFO:Assets written to: models/bert_uncased_trained5k_maxlen40_final\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"models/bert_uncased_trained5k_maxlen40_final\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da6071d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8931572437286377, 0.8974589705467224]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
